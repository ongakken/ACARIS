"""
The preprocess module contains methods for preprocessing text data.
"""

from transformers import AutoTokenizer
import os

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"



class Preprocessor:
    def __init__(self, mdl):
        self.tokenizer = AutoTokenizer.from_pretrained(mdl)

    def tokenize(self, text, maxLen=128, padding=True, truncation=True, returnTensors="pt"):
        """
        This function tokenizes text using a specified tokenizer and returns the resulting tokens with
        optional padding and truncation.
        
        @param text The input text that needs to be tokenized.
        @param maxLen The maximum length of the tokenized sequence. If the input sequence is longer than
        this, it will be truncated. If it is shorter, it will be padded.
        @param padding If set to True, the tokenizer will pad the sequences to the same length by adding
        special tokens (usually 0s) at the end of shorter sequences.
        @param truncation Truncation is a technique used to limit the length of the input sequence to a
        maximum length. If the input sequence is longer than the maximum length, it is truncated from the
        end. This is useful when dealing with sequences of varying lengths, as it allows us to process them
        in batches of fixed
        @param returnTensors The returnTensors parameter specifies the type of tensor to be returned by the
        tokenizer. It can take the values "pt" for PyTorch tensors or "tf" for TensorFlow tensors.
        
        @return a dictionary of tokens generated by the tokenizer function with the specified parameters.
        The dictionary contains the input_ids, attention_mask, and token_type_ids. The function also has an
        optional parameter to concatenate userEmbedding with the input_ids. The return type of the
        dictionary can be either PyTorch tensors or TensorFlow tensors depending on the value of the
        returnTensors parameter.
        """
        tokens = self.tokenizer(text, max_length=maxLen, padding=padding, truncation=truncation, return_tensors=returnTensors)
        #tokens["input_ids"] = torch.cat([userEmbedding, tokens["input_ids"]], dim=-1)
        #tokens["userID"] = torch.tensor(userID, dtype=torch.long)
        return tokens

if __name__ == "__main__":
    mdl = "distilbert-base-uncased"
    preprocessor = Preprocessor(mdl)
    text = "F* me, hard!"
    tokens = preprocessor.tokenize(text, 64)
    print(f"Sweet, sweet tokens:\n{tokens}")
    print(f"Decoded tokens:\n{preprocessor.tokenizer.decode(tokens['input_ids'][0])}")