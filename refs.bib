@misc{clark2018think,
  title         = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author        = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  year          = {2018},
  eprint        = {1803.05457},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@online{Cop_2021,
  title   = {Tweet sentiment and emotion analysis},
  url     = {https://www.kaggle.com/datasets/subhajournal/tweet-sentiment-and-emotion-analysis?select=all_tweets.csv},
  journal = {Kaggle},
  author  = {Cop, Cyber},
  year    = {2021},
  month   = {9}
}

@online{demszky2020goemotions,
  author    = {Demszky, Dorottya and Movshovitz-Attias, Dana and Ko, Jeongwoo and Cowen, Alan and Nemade, Gaurav and Ravi, Sujith},
  booktitle = {58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  title     = {{GoEmotions: A Dataset of Fine-Grained Emotions}},
  year      = {2020}
}

@online{devlin2019bert,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@software{eval-harness,
  author    = {Gao, Leo and
               Tow, Jonathan and
               Biderman, Stella and
               Black, Sid and
               DiPofi, Anthony and
               Foster, Charles and
               Golding, Laurence and
               Hsu, Jeffrey and
               McDonell, Kyle and
               Muennighoff, Niklas and
               Phang, Jason and
               Reynolds, Laria and
               Tang, Eric and
               Thite, Anish and
               Wang, Ben and
               Wang, Kevin and
               Zou, Andy},
  title     = {A framework for few-shot language model evaluation},
  month     = sep,
  year      = 2021,
  publisher = {Zenodo},
  version   = {v0.0.1},
  doi       = {10.5281/zenodo.5371628},
  url       = {https://doi.org/10.5281/zenodo.5371628}
}

@article{falcon40b,
  title  = {{Falcon-40B}: an open large language model with state-of-the-art performance},
  author = {Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year   = {2023}
}

@book{geron2022hands,
  title     = {Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow},
  author    = {G{\'e}ron, Aur{\'e}lien},
  year      = {2022},
  publisher = {" O'Reilly Media, Inc."}
}

@misc{hendrycks2021measuring,
  title         = {Measuring Massive Multitask Language Understanding},
  author        = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  year          = {2021},
  eprint        = {2009.03300},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CY}
}

@online{https://doi.org/10.1111/cogs.13085,
  author   = {Iordan, Marius Cătălin and Giallanza, Tyler and Ellis, Cameron T. and Beckage, Nicole M. and Cohen, Jonathan D.},
  title    = {Context Matters: Recovering Human Semantic Structure from Machine Learning Analysis of Large-Scale Text Corpora},
  journal  = {Cognitive Science},
  volume   = {46},
  number   = {2},
  pages    = {e13085},
  keywords = {Similarity, Semantic, Neural network, Context, Features, Text corpora},
  doi      = {https://doi.org/10.1111/cogs.13085},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13085},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13085},
  abstract = {Abstract Applying machine learning algorithms to automatically infer relationships between concepts from large-scale collections of documents presents a unique opportunity to investigate at scale how human semantic knowledge is organized, how people use it to make fundamental judgments (“How similar are cats and bears?”), and how these judgments depend on the features that describe concepts (e.g., size, furriness). However, efforts to date have exhibited a substantial discrepancy between algorithm predictions and human empirical judgments. Here, we introduce a novel approach to generating embeddings for this purpose motivated by the idea that semantic context plays a critical role in human judgment. We leverage this idea by constraining the topic or domain from which documents used for generating embeddings are drawn (e.g., referring to the natural world vs. transportation apparatus). Specifically, we trained state-of-the-art machine learning algorithms using contextually-constrained text corpora (domain-specific subsets of Wikipedia articles, 50+ million words each) and showed that this procedure greatly improved predictions of empirical similarity judgments and feature ratings of contextually relevant concepts. Furthermore, we describe a novel, computationally tractable method for improving predictions of contextually-unconstrained embedding models based on dimensionality reduction of their internal representation to a small number of contextually relevant semantic features. By improving the correspondence between predictions derived automatically by machine learning methods using vast amounts of data and more limited, but direct empirical measurements of human judgments, our approach may help leverage the availability of online corpora to better understand the structure of human semantic representations and how people make judgments based on those.},
  year     = {2022}
}

@online{https://doi.org/10.48550/arxiv.2204.11824,
  doi       = {10.48550/ARXIV.2204.11824},
  url       = {https://arxiv.org/abs/2204.11824},
  author    = {Blattmann, Andreas and Rombach, Robin and Oktay, Kaan and Ommer, Björn},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Retrieval-Augmented Diffusion Models},
  publisher = {arXiv},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{kamath2022transformers,
  title     = {Transformers for Machine Learning: A Deep Dive},
  author    = {Kamath, U. and Graham, K.L. and Emara, W.},
  isbn      = {9781000587098},
  url       = {https://books.google.de/books?id=zENpEAAAQBAJ},
  year      = {2022},
  publisher = {CRC Press}
}

@online{Kumar_2022,
  title   = {Twitter-sentiment data},
  url     = {https://www.kaggle.com/datasets/codersaurabh/twittersentiment-data?select=Tweet-Sentimental-Data.csv},
  journal = {Kaggle},
  author  = {Kumar, Saurabh},
  year    = {2022},
  month   = {7}
}

@misc{lin2022truthfulqa,
  title         = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author        = {Stephanie Lin and Jacob Hilton and Owain Evans},
  year          = {2022},
  eprint        = {2109.07958},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@online{liu2023visual,
  title         = {Visual Instruction Tuning},
  author        = {Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
  year          = {2023},
  eprint        = {2304.08485},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{open-llm-leaderboard,
  author       = {Edward Beeching and Sheon Han and Nathan Lambert and Nazneen Rajani and Omar Sanseviero and Lewis Tunstall and Thomas Wolf},
  title        = {Open LLM Leaderboard},
  year         = {2023},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}}
}

@book{raschka2022machine,
  title     = {Machine Learning with PyTorch and Scikit-Learn:
               Develop machine learning and deep learning models with Python},
  author    = {Raschka, Sebastian and Liu, Yuxi Hayden and Mirjalili, Vahid and Dzhulgakov, Dmytro},
  year      = {2022},
  publisher = {Packt Publishing Ltd}
}

@online{rombach2021highresolution,
  title         = {High-Resolution Image Synthesis with Latent Diffusion Models},
  author        = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
  year          = {2021},
  eprint        = {2112.10752},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@book{rothman2022transformers,
  title     = {Transformers for Natural Language Processing: Build, train, and fine-tune deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, and GPT-3},
  author    = {Rothman, D. and Gulli, A.},
  isbn      = {9781803243481},
  url       = {https://books.google.de/books?id=u9FjEAAAQBAJ},
  year      = {2022},
  publisher = {Packt Publishing}
}
@online{sanh2020distilbert,
  title         = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author        = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  year          = {2020},
  eprint        = {1910.01108},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{sharma2022humanai,
  title         = {Human-AI Collaboration Enables More Empathic Conversations in Text-based Peer-to-Peer Mental Health Support},
  author        = {Ashish Sharma and Inna W. Lin and Adam S. Miner and David C. Atkins and Tim Althoff},
  year          = {2022},
  eprint        = {2203.15144},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{vaswani2017attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{wandb,
  title = {Weights and Biases},
  url   = {https://wandb.ai/site}
}

@misc{zellers2019hellaswag,
  title         = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  author        = {Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
  year          = {2019},
  eprint        = {1905.07830},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
