\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage[backend=biber, sorting=none]{biblatex}
\addbibresource{refs.bib}
%\usepackage{multicol}

\title{ACARIS: Improving Conversational AI and Human Social Skills using User Embeddings}
\author{Simon Slamka\\
\small\textbf{OngakkenAI}}
\date{June 1, 2023}

\begin{document}

\maketitle

\begin{abstract}
In this paper, we propose ACARIS, the Advanced Communication Augmentor and Relational Insights System, a system utilizing a novel method to analyze emotional state, intent, and interest of text communication parties. ACARIS is being built with the goal of improving social skills of humans, while also improving the performance of human-facing AI systems. We go over our approach, including the initialization of user embeddings from message features, concatenation of user embeddings with word embeddings, modifications of the BERT architecture, and the training and evaluation processes. We also go over the results of our experiments, which demonstrate the effectiveness of our method.
\end{abstract}

\section{Introduction}
\subsection{Keywords}
ACARIS, Conversational AI, Social Skills, User Embeddings, BERT, DistilBERT, Sentiment Analysis, Intent Classification, Emotion Recognition, Interest Recognition

\subsection{Definitions}
\begin{itemize}
	\item \textbf{ACARIS} - the Advanced Communication Augmentor and Relational Insights System
	\item \textbf{NLP} - Natural Language Processing - a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data
	\item \textbf{Deep Learning} - a class of machine learning algorithms that uses multiple layers to progressively extract higher-level features from the raw input
	\item \textbf{SVM} - Support Vector Machine - a supervised machine learning model that uses classification algorithms for two-group classification problems
	\item \textbf{DT} - Decision Tree - 
	\item \textbf{LogReg} - Logistic Regression - a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist
	\item \textbf{RF} - Random Forest - an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees
	\item \textbf{TF-IDF} - Term Frequency - Inverse Document Frequency - a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus
	\item \textbf{the Transformer} - a deep learning architecture introduced in 2017, used primarily in the field of NLP\cite{vaswani2017attention}
	\item \textbf{BERT} - Bidirectional Encoder Representations from Transformers - a transformer-based machine learning model for natural language processing pre-training developed by Google\cite{devlin2019bert}
	\item \textbf{DistilBERT} - DistilBERT is a smaller, faster, cheaper version of BERT developed by HuggingFace\cite{sanh2020distilbert}
	\item \textbf{Vector} - a quantity that has both magnitude and direction
	\item \textbf{Vector Space} - a collection of vectors, which may be added together and multiplied ("scaled") by numbers, called scalars
	\item \textbf{Embedding Space} - a vector space with a coordinate for each word in the vocabulary, such that words that share common contexts in the corpus are located close to one another in the space
	\item \textbf{Word Embedding} - A vector representation of a word's meaning
	\item \textbf{User Embedding} - A vector representation of a user's personality, emotional state, intent, and interest
	\item \textbf{ReLU} - Rectified Linear Unit - an activation function that returns the input value if it is positive, otherwise it returns zero
	\item \textbf{Loss} - a number indicating how bad the model's prediction was on a single example
	\item \textbf{Cross-Entropy Loss} - a loss function used for classification problems
	\item \textbf{Adam} - an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights
	\item \textbf{Ablation Study} - a study in which specific components of a system are removed to analyze their impact on the overall performance
\end{itemize}
\subsection{Motivation}
I've always had an issue with interpersonal relationships. I could never fully understand the way people work on a social level. I understand the technical and biological fundamentals, and superficially, how the mind works, but not how all this becomes so much more complex when actually talking and dealing with people. Sometimes, I feel that another person and I broadcast on different frequencies, because we are unable to either understand one another or maintain a (romantic) relationship for an extended period of time. I have tried for many years to find a technical solution to this problem, but only after coming to Denmark, I acquired the core knowledge to try and accomplish this ambitious goal by really getting into it and self-studying as much as I could.\\
I firmly believe that all things in nature are governed by rules and these rules can be observed, measured, and then, based on that data, predicted. This includes human behavior and emotions. We’re nothing more than complex electrobiochemical machines driven by electrical impulses and hormones. Apart from that, even if we don’t understand our own minds yet, we still have many, many years of knowledge about how human personalities work and how we make decisions based on what happens to or around us. On a fundamental level, this doesn’t change. Sure, they say that we’re all different. However, the core remains. We all share many attributes that make us human. I believe that we, given enough data, can use these attributes to predict human behavior, reactions, emotional state, and intent.

\subsection{Hypothesis}
Given enough conversational data per person, human behavior (emotional state, intent) in text communication can be predicted with a high level of confidence ($>80\%$) due to the fact that humans are, on a fundamental level, very similar to one another.

Additionally, we posit that person-specific performance improvements can be achieved by individualizing predictions by using a unique vector representation of a person's personality, emotional state, intent, and interest, which we call a user embedding. The postulate stems in the core concept of neural networks being universal function approximators. Therefore, in theory, a neural network should be able to learn to associate a user embedding with a person's behavior in text communication and use that information to improve its predictions.

%\begin{multicols}{2}
\subsection{Premise}
Interpersonal communication has always been an integral part of human lives and is critical from the moment we're born. With the rise of the Internet and, subsequently, social media and other forms of online text communication, the manner in which we talk has changed dramatically. This has led to a drop in social skills in humans, particularly those of the last generation. ACARIS attempts to adjust for this by providing a way to analyze the emotional state, intent, and interest of text communication parties, providing them with a way to improve their social skills, while also improving the performance of conversational AI systems, which are becoming increasingly prevalent in our society, and their ability to understand human emotions, intent, and interest is becoming more and more important, especially in AI systems that directly interact with humans, such as digital assistants, chatbots, and others.

\subsection{Literature Review and Related Work}
\subsubsection{Studied Literature}
\begin{itemize}
	\item \textbf{Machine Learning with PyTorch and Scikit-Learn}\cite{raschka2022machine} by Yuxi Liu, Vahid Mirjalili, Sebastian Raschka
	\item \textbf{Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow}\cite{geron2022hands} by Aurélien Géron
	\item \textbf{Transformers for Natural Language Processing}\cite{rothman2022transformers} by Denis Rothman, Antonio Gulli
\end{itemize}
\subsubsection{Related Work}
\begin{itemize}
	\item \textbf{Human-AI Collaboration Enables More Empathic Conversations in Text-based Peer-to-Peer Mental Health Support}\cite{sharma2022humanai}
	\item \textbf{Context Matters: Recovering Human Semantic Structure from Machine Learning Analysis of Large-Scale Text Corpora}\cite{https://doi.org/10.1111/cogs.13085}
\end{itemize}

\subsection{Initial Attempts}
Our initial attempts of implementing ACARIS included the use of SVMs, DTs, LogReg, and RFs, each paired with TF-IDF in the preprocessing stage. None of these methods have proven to be accurate enough (barely reaching 50\% accuracy) for large amounts of complex language, especially casual language that often includes slang, sarcasm, emoji, and similar elements. The primary issue was the lack of these methods' ability to capture semantical and contextual aspects of our dataset. SVMs, DTs, LogReg, and RFs weren't built specifically for sequential data. For sequential data, RNNs and CNNs were classically used. However, due to the long-term dependency problem of RNNs, we made the decision to not use them, as we believed that they would not be able to capture the long-term dependencies in our dataset. Additionally, we weren't able to guarantee that during inference, there would be no long-term dependencies.
Another issue was TF-IDF's lack of word order understanding. Knowing that the Transformer\cite{vaswani2017attention} architecture is capable of capturing these aspects due to its positional encodinds and self-attention mechanism, we decided to use it as the basis for ACARIS.

\subsection{Short Intro on Transformers}
The Transformer is a deep learning model proposed in 2017 by a team of Google Brain/Google Research researchers. It is mainly used in NLP, but it has also found its way into numerous vision projects (Latent Diffusion\cite{rombach2021highresolution}/Stable Diffusion/LLaVA\cite{liu2023visual}, to name a few).

\section{Methodology}
In this section, we provide a detailed description of our approach, including the computation of user embeddings, concatenation with word embeddings, modifications to the model architecture, and the training and evaluation processes.

% features:
% - Mean wordcount (per msg)
% - Vocab richness (type-token ratio?) - Unique words / total words
% - Mean emoji count (per msg)
% - Mean emoticon count (per msg)
% - Mean punctuation count (per msg)
% - Mean sentiment score (per msg)
% - Dominant/prevalent topics
% - Mean response time (!exclude closing messages, such as "Goodnight")
% - Mean message count (per day)
% - Mean links (per msg)
% - Mean markdown code snippet sections (per msg)
% - Mean abbreviations and acronyms (per msg) (such as "lol", "omg", "FBI", ...)
% - Mean hashtag count (per msg)

\subsection{Dataset}
To build our dataset, we collected 100 thousand messages from our Discord server, classified them into 3 classes (pos, neg, neu), and augmented them with the GoEmotions\cite{demszky2020goemotions} dataset, which contains 58 thousand messages classified into 27 classes. We squashed the 27 classes into 3 classes (pos, neg, neu) and concatenated the two datasets. To minimize error caused by input data, we removed all users with less than 25 messages, URLs, markdown code blocks, Discord mentions and custom emoji, and attachments. The resulting dataset was split into 75\% training, 10\% validation, and 15\% test sets.

\subsection{User Embeddings Initialization}
For each user $u$, we loop through all messages $m$ sent by $u$ and extract these features:
\begin{itemize}
	\item \textbf{Mean word count (per all $m$)} - \(\overline{w} = \frac{1}{N} \sum_{i=1}^{N} w_i \), where $w_i$ is the word count of $i$-th message and $N$ is the total number of messages
	\item \textbf{Vocabulary richness (per all $m$)} - \(r = \frac{U}{W} \), where $U$ is the number of unique words and $W$ is the total number of words
	\item \textbf{Mean emoji count (per all $m$)} - \(\overline{e} = \frac{1}{N} \sum_{i=1}^{N} e_i \), where $e_i$ is the emoji count of $i$-th message and $N$ is the total number of messages
	\item \textbf{Mean emoticon count (per all $m$)} - \(\overline{em} = \frac{1}{N} \sum_{i=1}^{N} em_i \), where $e_i$ is the emoticon count of $i$-th message and $N$ is the total number of messages
	\item \textbf{Mean punctuation count (per all $m$)} - \(\overline{p} = \frac{1}{N} \sum_{i=1}^{N} p_i \), where $p_i$ is the punctuation count of $i$-th message and $N$ is the total number of messages
	\item \textbf{Mean sentiment score (per all $m$)} - \(\overline{s} = \frac{1}{N} \sum_{i=1}^{N} s_i \), where $s_i$ is the sentiment score of $i$-th message and $N$ is the total number of messages
	\item \textbf{Dominant/prevalent topics (per all $m$)} - the most common topics in all messages
	\item \textbf{Mean response time (per all $m$)} - \(\overline{r} = \frac{1}{N} \sum_{i=1}^{N} r_i \), where $r_i$ is the response time of $i$-th message and $N$ is the total number of messages
	\item \textbf{Mean message count (per day)} - \(\overline{m} = \frac{1}{D} \sum_{i=1}^{D} m_i \), where $m_i$ is the message count of $i$-th day and $D$ is the total number of days
	\item \textbf{Mean links (per all $m$)} - \(\overline{l} = \frac{1}{N} \sum_{i=1}^{N} l_i \), where $l_i$ is the link count of $i$-th message and $N$ is the total number of messages
	\item \textbf{Mean markdown code snippet sections (per all $m$)} - \(\overline{c} = \frac{1}{N} \sum_{i=1}^{N} c_i \), where $c_i$ is the markdown code snippet section count of $i$-th message and $N$ is the total number of messages
	\item \textbf{Mean abbreviations and acronyms (per all $m$)} - \(\overline{a} = \frac{1}{N} \sum_{i=1}^{N} a_i \), where $a_i$ is the abbreviation and acronym count of $i$-th message and $N$ is the total number of messages
	\item \textbf{Mean hashtag count (per all $m$)} - \(\overline{h} = \frac{1}{N} \sum_{i=1}^{N} h_i \), where $h_i$ is the hashtag count of $i$-th message and $N$ is the total number of messages
\end{itemize}
These features were selected based on what we thought would most accurately represent a user's text communication behavior. We projected that no two users would have the same or closely similar values for all of these features, which would allow us to differentiate between them.
Then, we concatenate all the features into a single vector $e_u$ (of size $d_e$):
\begin{equation}
\begin{aligned}
&\text{\texttt{from torch import cat}} \\
&e_u = \textnormal{\texttt{cat}}([\overline{w}, r, \overline{e}, \overline{em}, \overline{p}, \overline{s}, \overline{r}, \overline{m}, \overline{l}, \overline{c}, \overline{a}, \overline{h}], \textnormal{dim}=-1)
\end{aligned}
\end{equation}
\subsection{Concatenation of User and Word Embeddings}
Given a message $m$ with its word embedding $e_w$ (of size $d_m$) from a BERT-like pre-trained model, which, in our case, was DistilBERT\cite{sanh2020distilbert}, we concatenate the user embedding $e_u$ to the word embedding $e_w$ to create a joint representation $e_\textnormal{joint}$ (of size $d_e + d_m$):
\begin{equation}
\begin{aligned}
&\text{\texttt{from torch import cat}} \\
&e_\textnormal{joint} = \textnormal{\texttt{cat}}([e_u, e_w], \textnormal{dim}=-1)
\end{aligned}
\end{equation}

\subsection{Model Architecture Modification}
To accommodate the concatenated embeddings, we create a custom input layer with the new input size $d_e + d_m$ within a BERT-flavored model. An alternative that we thought of using was to use a separate fully-connected layer (with ReLU activation) to map the concatenated embeddings to a compatible size:
\begin{equation}
\begin{aligned}
&\text{\texttt{from torch import randn}} \\
&\text{\texttt{from torch import relu}} \\
&\text{\texttt{from torch import matmul}} \\
&W1 = \text{\texttt{randn}}((d_e + d_m, d_m)) \\
&b1 = \text{\texttt{randn}}(d_m) \\
&e_\textnormal{map} = \text{\texttt{relu}}(\text{\texttt{matmul}}(e_\textnormal{joint}, W1) + b1)
\end{aligned}
\end{equation}
Then, pass $e_\textnormal{map}$ through the existing model architecture.

\subsection{Training with User Embeddings}
For each training step, we pass the concatenated embeddings $e_\textnormal{joint}$ through the model and compute the cross-entropy loss $L$. Then, we backpropagate the gradients from the loss to the model parameters and update the parameters using Adam. The model thus learns to associate every message $m$ with a user embedding $e_u$ and a sentiment label $y$.

\subsection{Updating User Embeddings}
The user embeddings $e_u$ are not updated during training. Instead, they are pre-computed during data preprocessing and stored in an embedding matrix. We repeat training every week, using new data from the previous week to update the user embeddings $e_u$. The reason for this decision is that we want the user embeddings $e_u$ in a static state to prevent any potential issues with losses. We considered updating the user embeddings $e_u$ during training, but we decided against it because we believe that it would be too computationally expensive and would not provide any significant benefits. It could actually worsen the model's performance, as the user embeddings $e_u$ would be constantly changing, which could lead to the model being unable to learn to associate messages $m$ with user embeddings $e_u$.

\section{Evaluation}
NaN

\section{Validation}
To validate our approach, we compared the performance of our model with the performance of a baseline model. We used the same model, DistilBERT, but without the user embeddings $e_u$. We fine-tuned both models on the same dataset and compared their performance on the same test set.
\section{Results}
NaN

\section{Conclusion}
NaN

\printbibliography

%\end{multicols}

\end{document}